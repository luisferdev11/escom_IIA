{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practica 4. Identificación de palabras, frases y documentos similares\n",
        "**Tecnologías de Lenguaje Natural**\n",
        "\n",
        "*Luis Fernando Rodríguez Domínguez*\n",
        "\n",
        "5BV1\n",
        "\n",
        "*Ingeniería en Inteligencia Artificial*\n",
        "\n",
        "Fecha última de modificación: 14 de mayo del 2025"
      ],
      "metadata": {
        "id": "7T_9mdVHFZzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalidad del Programa\n",
        "\n",
        "Este programa en Python, presentado como un Jupyter Notebook, tiene como objetivo principal desarrollar un sistema capaz de identificar y cuantificar la similitud entre palabras, frases y documentos. Para ello, se aplicarán diversas técnicas de Procesamiento de Lenguaje Natural (NLP), abarcando tanto enfoques semánticos basados en recursos léxicos estructurados (WordNet) como enfoques sintácticos y semánticos basados en representaciones vectoriales densas (embeddings como GloVe y BERT).\n",
        "\n",
        "El flujo de trabajo incluye:\n",
        "1.  La creación de un corpus a partir de las introducciones de cinco libros con temáticas similares obtenidos del Proyecto Gutenberg.\n",
        "2.  La normalización de estos documentos, incluyendo segmentación, tokenización, etiquetado gramatical y otras técnicas de limpieza.\n",
        "3.  El cálculo de similitud entre palabras (verbos y sustantivos más frecuentes) utilizando métricas de WordNet y embeddings GloVe.\n",
        "4.  La extracción de frases representativas de cada documento y el cálculo de similitud entre ellas utilizando WordNet y embeddings BERT.\n",
        "5.  Un análisis comparativo de los resultados obtenidos mediante los diferentes enfoques.\n",
        "\n",
        "## Datos de entrada\n",
        "\n",
        "Los principales datos de entrada manejados en este programa son:\n",
        "\n",
        "*   **Archivos de Texto (`.txt`):** Se requieren 5 archivos de texto, cada uno conteniendo la introducción de un libro. Estos archivos deben estar ubicados en el mismo directorio que el notebook y nombrados siguiendo un patrón (ej. `doc1.txt`, `doc2.txt`, ..., `doc5.txt`) para que `PlaintextCorpusReader` los pueda identificar.\n",
        "*   **Corpus NLTK:** Se crea un objeto `PlaintextCorpusReader` para manejar los documentos de texto como un corpus.\n",
        "*   **Listas y Strings:** Para almacenar textos, oraciones, tokens, y resultados.\n",
        "*   **Diccionarios:** Para almacenar frecuencias de palabras, embeddings, y documentos etiquetados.\n",
        "*   **Synsets de WordNet:** Objetos especiales de NLTK que representan conjuntos de sinónimos y conceptos léxicos.\n",
        "*   **Vectores Numéricos (Embeddings):** Arrays de NumPy que representan palabras o frases en un espacio vectorial, generados por GloVe y BERT.\n",
        "*   **Modelos Pre-entrenados:**\n",
        "    *   **GloVe:** Se utiliza el archivo `glove.6B.50d.txt` (Wikipedia 2014 + Gigaword 5, vectores de 50 dimensiones). Este archivo debe descargarse de [Stanford NLP GloVe](https://nlp.stanford.edu/projects/glove/) y colocarse en el directorio de trabajo.\n",
        "    *   **BERT:** Se utiliza el modelo `bert-base-uncased` de Hugging Face, que se descarga automáticamente a través de la biblioteca `transformers`.\n",
        "\n",
        "## Listado y Descripción de Funciones\n",
        "\n",
        "A continuación, se listan las funciones principales desarrolladas en este notebook, con una breve descripción de su propósito. Las descripciones detalladas se encuentran junto a la definición de cada función.\n",
        "\n",
        "*   `get_wordnet_pos(treebank_tag)`: Convierte etiquetas gramaticales del formato Penn Treebank al formato compatible con WordNet.\n",
        "*   `normalizar_documento_wordnet(texto_crudo, lematizador, stop_words_set)`: Procesa un texto crudo para obtener una lista de tuplas (lema, etiqueta_wordnet), adecuada para trabajar con WordNet.\n",
        "*   `obtener_palabras_similares_wordnet(palabra_base, tipo_palabra_wn, top_n=5)`: Encuentra las `top_n` palabras más similares a `palabra_base` usando `path_similarity` y `wup_similarity` de WordNet.\n",
        "*   `extraer_frase_representativa_textrank(texto_documento, num_oraciones=1)`: Extrae la frase (u oraciones) más representativa(s) de un texto usando el algoritmo TextRank.\n",
        "*   `doc_to_synsets_wordnet(document_text, lematizador, stop_words_set)`: Convierte un texto en una lista de synsets de WordNet, filtrando y lematizando.\n",
        "*   `path_similarity_entre_synsets(lista_synsets1, lista_synsets2)`: Calcula la similitud promedio basada en `path_similarity` entre dos listas de synsets.\n",
        "*   `document_path_similarity_wordnet(doc_text1, doc_text2, lematizador, stop_words_set)`: Calcula la similitud entre dos documentos usando `path_similarity` sobre sus synsets.\n",
        "*   `cargar_glove_model(glove_file_path)`: Carga los embeddings de GloVe desde un archivo.\n",
        "*   `find_closest_glove_embeddings(palabra, embeddings_dict, top_n=5)`: Encuentra las `top_n` palabras más similares a `palabra` usando embeddings GloVe y similitud coseno.\n",
        "*   `get_bert_sentence_embedding(sentence, bert_tokenizer, bert_model)`: Obtiene el embedding de una frase usando un modelo BERT.\n"
      ],
      "metadata": {
        "id": "x0ONjXFUFlXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencias"
      ],
      "metadata": {
        "id": "7BnllZ-jF8AA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnokdruAESEf",
        "outputId": "69f3f7c6-e0b0-40fb-992d-e82f2673e776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: sumy in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.11/dist-packages (from sumy) (24.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.4.26)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install numpy scipy scikit-learn\n",
        "!pip install sumy\n",
        "!pip install transformers torch\n",
        "#!pip install gensim # Opcional, si se quiere usar gensim para cargar GloVe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scipy --upgrade --force-reinstall"
      ],
      "metadata": {
        "id": "xm-WKVTWHLqp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "lQXjgNYGGDrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine # Para similitud coseno\n",
        "import matplotlib.pyplot as plt # Para visualizaciones futuras, si se añaden\n",
        "\n",
        "# NLTK (Natural Language Toolkit)\n",
        "from nltk.corpus import PlaintextCorpusReader, stopwords, wordnet as wn\n",
        "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Sumy (para resumen con TextRank)\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "# Transformers (para BERT)\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "rFtfu_mvGC8U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descarga de recursos adicionales necesarios"
      ],
      "metadata": {
        "id": "yKlc9vHNGOjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] Descargando recursos de NLTK necesarios...\")\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4') # Open Multilingual Wordnet\n",
        "\n",
        "print(\"[SUCCESS] Recursos de NLTK verificados/descargados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmk8nJGXGYvL",
        "outputId": "2ecdba92-2517-49d1-b924-20b042f2908a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Descargando recursos de NLTK necesarios...\n",
            "[SUCCESS] Recursos de NLTK verificados/descargados.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Generación de cuerpo de documentos (0 pts)\n",
        "\n",
        "El primer paso consiste en generar un corpus de documentos. Para esta práctica, se seleccionarán las **introducciones** de 5 libros del portal [Project Gutenberg](https://www.gutenberg.org/). Es importante que los libros pertenezcan a géneros o temas similares para que el análisis de similitud sea más significativo.\n",
        "\n",
        "**Instrucciones para el alumno:**\n",
        "1.  Visita [Project Gutenberg](https://www.gutenberg.org/).\n",
        "2.  Identifica 5 libros de un género o tema de tu interés (ej., ciencia ficción, filosofía, historia de la música, etc.).\n",
        "3.  Para cada libro, copia el texto correspondiente a su **introducción** (o prefacio, prólogo, o las primeras páginas si no hay una introducción formal).\n",
        "4.  Guarda cada introducción en un archivo de texto plano (`.txt`) separado. Nombra los archivos como `doc1.txt`, `doc2.txt`, `doc3.txt`, `doc4.txt`, y `doc5.txt`.\n",
        "5.  Coloca estos 5 archivos en el mismo directorio donde se encuentra este Jupyter Notebook.\n",
        "\n",
        "Una vez que los archivos estén listos, el siguiente código los cargará utilizando `PlaintextCorpusReader` de NLTK."
      ],
      "metadata": {
        "id": "8Zac7aBcGzcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_directory = '/content/'\n",
        "file_pattern = r'doc[1-5]\\.txt' # Expresión regular para encontrar los archivos\n",
        "\n",
        "# --- Inicialización del Tokenizer de Oraciones ---\n",
        "# Se recomienda PunktSentenceTokenizer para una segmentación de oraciones robusta.\n",
        "sentence_tokenizer = PunktSentenceTokenizer()\n",
        "\n",
        "# --- Creación del Corpus ---\n",
        "# PlaintextCorpusReader permite leer una colección de archivos de texto plano.\n",
        "try:\n",
        "    corpus = PlaintextCorpusReader(\n",
        "        corpus_directory,\n",
        "        file_pattern,\n",
        "        sent_tokenizer=sentence_tokenizer,\n",
        "        encoding='utf-8' # Especificar encoding por si acaso\n",
        "    )\n",
        "    print(f\"[SUCCESS] Corpus cargado exitosamente desde '{corpus_directory}'.\")\n",
        "    print(f\"Archivos identificados en el corpus: {corpus.fileids()}\")\n",
        "\n",
        "    # --- Verificación Inicial del Contenido ---\n",
        "    if not corpus.fileids():\n",
        "        print(\"[ERROR] No se encontraron archivos en el corpus. Verifica los nombres y la ubicación de tus archivos .txt.\")\n",
        "    else:\n",
        "        print(\"\\n--- Evidencia de Carga: Primera oración de cada documento ---\")\n",
        "        for file_id in corpus.fileids():\n",
        "            try:\n",
        "                first_sentence = corpus.sents(file_id)[0]\n",
        "                print(f\"Documento '{file_id}': {' '.join(first_sentence)[:100]}...\") # Muestra los primeros 100 caracteres\n",
        "            except IndexError:\n",
        "                print(f\"Documento '{file_id}': No se pudo extraer la primera oración (¿archivo vacío o sin segmentar?).\")\n",
        "            except Exception as e:\n",
        "                print(f\"Documento '{file_id}': Error al procesar - {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Ocurrió un error al intentar cargar el corpus: {e}\")\n",
        "    print(\"Por favor, asegúrate de que los archivos .txt (doc1.txt, ..., doc5.txt) existen en el directorio actual.\")\n",
        "    corpus = None # Asegurar que corpus es None si falla la carga\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcvqPNiHG0E9",
        "outputId": "b23a50e4-04c3-4447-d0a2-6734e87ee584"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] Corpus cargado exitosamente desde '/content/'.\n",
            "Archivos identificados en el corpus: ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt']\n",
            "\n",
            "--- Evidencia de Carga: Primera oración de cada documento ---\n",
            "Documento 'doc1.txt': It may be interesting to some persons to learn how it came about that Vatsyayana was first brought t...\n",
            "Documento 'doc2.txt': Leopold von Sacher - Masoch was born in Lemberg , Austrian Galicia , on January 27 , 1836 ....\n",
            "Documento 'doc3.txt': Son of a merchant , Boccaccio di Chellino di Buonaiuto , of Certaldo in Val d ' Elsa , a little town...\n",
            "Documento 'doc4.txt': The amatory motif is pervasive , timeless , and universal ....\n",
            "Documento 'doc5.txt': In reading several dozen books on sex matters for the young with a view to selecting the best for my...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Normalización de documentos (10 pts)\n",
        "\n",
        "La normalización es un paso crucial en NLP para reducir el ruido y estandarizar el texto, lo que mejora la calidad de los análisis posteriores. En esta sección, cada documento se segmentará en oraciones y luego en tokens. A cada token se le asignará su categoría gramatical (Part-of-Speech, POS).\n",
        "\n",
        "**Justificación de las Técnicas de Normalización:**\n",
        "\n",
        "Las técnicas de normalización se aplicarán según las necesidades de cada tarea (puntos 3 a 6):\n",
        "\n",
        "*   **Para tareas basadas en WordNet (Puntos 3 y 4):**\n",
        "    *   **Tokenización:** Dividir el texto en palabras individuales.\n",
        "    *   **Conversión a Minúsculas:** Estandariza las palabras (ej., \"Verbo\" y \"verbo\" se tratan igual).\n",
        "    *   **Eliminación de Puntuación y No Alfabéticos:** Los signos de puntuación y caracteres no alfabéticos generalmente no aportan significado léxico para WordNet.\n",
        "    *   **Eliminación de Stopwords:** Palabras comunes (ej., \"el\", \"es\", \"un\") que pueden no ser significativas para análisis semántico y pueden eliminarse para reducir el ruido.\n",
        "    *   **POS Tagging:** Identificar la categoría gramatical de cada palabra (sustantivo, verbo, adjetivo, etc.). Esto es esencial para WordNet, ya que los synsets son específicos de una categoría gramatical.\n",
        "    *   **Lematización:** Reducir las palabras a su forma base o lema (ej., \"corriendo\" -> \"correr\"). WordNet trabaja con lemas.\n",
        "\n",
        "*   **Para tareas basadas en Embeddings (Puntos 5 y 6):**\n",
        "    *   **GloVe (Punto 5):**\n",
        "        *   **Tokenización y Minúsculas:** GloVe suele estar pre-entrenado con texto en minúsculas.\n",
        "        *   La eliminación de stopwords y puntuación depende de cómo fue entrenado el modelo GloVe. Los modelos GloVe estándar suelen incluir stopwords y cierta puntuación, por lo que puede ser beneficioso mantenerlos si están en el vocabulario del modelo. Para este ejercicio, mantendremos un preprocesamiento similar al de WordNet (minúsculas, alfabéticos) para los términos objetivo (verbos más frecuentes) y luego buscaremos estos términos limpios en GloVe.\n",
        "    *   **BERT (Punto 6):**\n",
        "        *   BERT utiliza su propio **tokenizer especializado** (ej., WordPiece). Este tokenizer maneja la segmentación en sub-palabras, mayúsculas/minúsculas (si el modelo es \"uncased\"), y puntuación. Por lo tanto, a BERT se le deben pasar las frases con un preprocesamiento mínimo (idealmente, las oraciones originales).\n",
        "\n",
        "**Implementación de la Normalización General:**\n",
        "\n",
        "Primero, definiremos funciones auxiliares y realizaremos un pre-procesamiento que será la base para las tareas.\n"
      ],
      "metadata": {
        "id": "CFHFw_q2Ja_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words_english = set(stopwords.words('english'))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"\n",
        "    Convierte una etiqueta POS del formato Penn Treebank (usado por nltk.pos_tag)\n",
        "    al formato compatible con WordNetLemmatizer y WordNet synsets.\n",
        "    \"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return None # Para otras etiquetas como determinantes, preposiciones, etc.\n",
        "\n",
        "def normalizar_documento_wordnet(texto_crudo, lematizador_obj, stop_words_set):\n",
        "    \"\"\"\n",
        "    Normaliza un texto para análisis con WordNet.\n",
        "    Tokeniza, convierte a minúsculas, filtra no alfabéticos y stopwords,\n",
        "    realiza POS tagging y lematiza.\n",
        "\n",
        "    Args:\n",
        "        texto_crudo (str): El texto original del documento.\n",
        "        lematizador_obj (WordNetLemmatizer): Instancia del lematizador.\n",
        "        stop_words_set (set): Conjunto de stopwords a eliminar.\n",
        "\n",
        "    Returns:\n",
        "        list: Una lista de tuplas (lema, etiqueta_wordnet) para cada palabra procesada.\n",
        "    \"\"\"\n",
        "    tokens_procesados = []\n",
        "    # Segmentar en oraciones y luego en palabras para un POS tagging más contextual\n",
        "    for sent in nltk.sent_tokenize(texto_crudo):\n",
        "        words = nltk.word_tokenize(sent)\n",
        "        tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "        for word, tag in tagged_words:\n",
        "            word_lower = word.lower()\n",
        "            # Filtrar stopwords y no alfabéticos ANTES de lematizar\n",
        "            if word_lower.isalpha() and word_lower not in stop_words_set:\n",
        "                wn_tag = get_wordnet_pos(tag)\n",
        "                if wn_tag: # Solo lematizar si es N, V, ADJ, ADV\n",
        "                    lema = lematizador_obj.lemmatize(word_lower, pos=wn_tag)\n",
        "                    tokens_procesados.append((lema, wn_tag))\n",
        "    return tokens_procesados\n",
        "\n",
        "# --- Procesamiento de cada documento del corpus ---\n",
        "documentos_normalizados_wordnet = {} # Almacenará los tokens lematizados y etiquetados para WordNet\n",
        "\n",
        "if corpus:\n",
        "    print(\"\\n--- Normalizando documentos para análisis con WordNet ---\")\n",
        "    for file_id in corpus.fileids():\n",
        "        print(f\"Procesando '{file_id}'...\")\n",
        "        texto_original = corpus.raw(file_id)\n",
        "        documentos_normalizados_wordnet[file_id] = normalizar_documento_wordnet(texto_original, lemmatizer, stop_words_english)\n",
        "\n",
        "        # Evidencia de normalización (primeros 10 tokens)\n",
        "        if documentos_normalizados_wordnet[file_id]:\n",
        "             print(f\"Primeros 10 tokens normalizados para '{file_id}': {documentos_normalizados_wordnet[file_id][:10]}\")\n",
        "        else:\n",
        "            print(f\"'{file_id}' no produjo tokens normalizados (¿contenido filtrado completamente?).\")\n",
        "    print(\"[SUCCESS] Normalización para WordNet completada.\")\n",
        "else:\n",
        "    print(\"[ERROR] El corpus no está cargado. No se puede proceder con la normalización.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch98_NiuJbtc",
        "outputId": "6055a2a9-e226-4726-d4dc-a7bfea348b48"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Normalizando documentos para análisis con WordNet ---\n",
            "Procesando 'doc1.txt'...\n",
            "Primeros 10 tokens normalizados para 'doc1.txt': [('interest', 'v'), ('person', 'n'), ('learn', 'v'), ('come', 'v'), ('vatsyayana', 'n'), ('first', 'r'), ('bring', 'v'), ('light', 'n'), ('translate', 'v'), ('english', 'a')]\n",
            "Procesando 'doc2.txt'...\n",
            "Primeros 10 tokens normalizados para 'doc2.txt': [('leopold', 'a'), ('von', 'n'), ('bear', 'v'), ('lemberg', 'n'), ('austrian', 'n'), ('galicia', 'n'), ('january', 'n'), ('study', 'v'), ('jurisprudence', 'n'), ('prague', 'n')]\n",
            "Procesando 'doc3.txt'...\n",
            "Primeros 10 tokens normalizados para 'doc3.txt': [('son', 'n'), ('merchant', 'n'), ('boccaccio', 'n'), ('chellino', 'n'), ('buonaiuto', 'n'), ('certaldo', 'n'), ('val', 'n'), ('little', 'a'), ('town', 'n'), ('midway', 'n')]\n",
            "Procesando 'doc4.txt'...\n",
            "Primeros 10 tokens normalizados para 'doc4.txt': [('amatory', 'n'), ('motif', 'n'), ('pervasive', 'a'), ('timeless', 'n'), ('universal', 'n'), ('phase', 'n'), ('manifestation', 'n'), ('present', 'v'), ('provocation', 'n'), ('infrequently', 'r')]\n",
            "Procesando 'doc5.txt'...\n",
            "Primeros 10 tokens normalizados para 'doc5.txt': [('read', 'v'), ('several', 'a'), ('dozen', 'n'), ('book', 'n'), ('sex', 'n'), ('matter', 'n'), ('young', 'a'), ('view', 'n'), ('select', 'v'), ('best', 'a')]\n",
            "[SUCCESS] Normalización para WordNet completada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Similitud de palabras con synsets (10 pts)\n",
        "\n",
        "En esta sección, para cada documento:\n",
        "1.  Se identificará el **verbo más común** y el **sustantivo más frecuente**.\n",
        "2.  Se hallarán los 5 términos más parecidos a ese verbo (y luego al sustantivo) usando dos métricas de similitud de WordNet:\n",
        "    *   `wup_similarity` (Wu-Palmer similarity)\n",
        "    *   `path_similarity`\n",
        "3.  En total, se obtendrán 10 verbos/sustantivos similares por cada verbo/sustantivo frecuente (5 con cada métrica).\n",
        "\n",
        "La lematización realizada en el paso anterior es crucial aquí, ya que WordNet opera sobre lemas."
      ],
      "metadata": {
        "id": "iEb7gLcbJ5IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_palabras_similares_wordnet(palabra_base, tipo_palabra_wn, top_n=5):\n",
        "    \"\"\"\n",
        "    Encuentra las 'top_n' palabras más similares a 'palabra_base' de un 'tipo_palabra_wn'\n",
        "    específico (ej. wn.VERB, wn.NOUN) usando path_similarity y wup_similarity de WordNet.\n",
        "\n",
        "    Args:\n",
        "        palabra_base (str): El lema de la palabra para la cual buscar similares.\n",
        "        tipo_palabra_wn (str): El tipo de palabra según WordNet (wn.VERB, wn.NOUN, etc.).\n",
        "        top_n (int): El número de palabras similares a retornar por cada métrica.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Dos listas de tuplas (palabra_similar, similitud).\n",
        "               La primera lista es para path_similarity, la segunda para wup_similarity.\n",
        "               Cada tupla contiene el lema de la palabra similar y su puntuación de similitud.\n",
        "    \"\"\"\n",
        "    similares_path = []\n",
        "    similares_wup = []\n",
        "\n",
        "    # Obtener el primer synset de la palabra base (el más común)\n",
        "    synsets_base = wn.synsets(palabra_base, pos=tipo_palabra_wn)\n",
        "    if not synsets_base:\n",
        "        # print(f\"Advertencia: No se encontraron synsets para '{palabra_base}' como {tipo_palabra_wn}.\")\n",
        "        return [], []\n",
        "\n",
        "    synset_base_principal = synsets_base[0]\n",
        "\n",
        "    # Comparar con todos los otros synsets del mismo tipo de palabra\n",
        "    for synset_candidato in wn.all_synsets(pos=tipo_palabra_wn):\n",
        "        if synset_candidato == synset_base_principal:\n",
        "            continue # No comparar consigo mismo\n",
        "\n",
        "        # Calcular similitudes\n",
        "        path_sim = synset_base_principal.path_similarity(synset_candidato)\n",
        "        wup_sim = synset_base_principal.wup_similarity(synset_candidato)\n",
        "\n",
        "        # Almacenar si la similitud es válida\n",
        "        if path_sim is not None:\n",
        "            # Un synset puede tener múltiples lemas, tomamos el primero como representativo\n",
        "            for lema in synset_candidato.lemmas():\n",
        "                similares_path.append(((lema.name(), synset_candidato.name()), path_sim))\n",
        "                break # Solo el primer lema del synset candidato\n",
        "\n",
        "        if wup_sim is not None:\n",
        "            for lema in synset_candidato.lemmas():\n",
        "                similares_wup.append(((lema.name(), synset_candidato.name()), wup_sim))\n",
        "                break # Solo el primer lema del synset candidato\n",
        "\n",
        "    # Eliminar duplicados por nombre de lema, conservando la mayor similitud\n",
        "    # (puede haber múltiples synsets para un mismo lema, o lemas que dan el mismo nombre)\n",
        "    def unique_sorted_lemmas(sim_list):\n",
        "        lemma_sim_dict = {}\n",
        "        for (lemma_name, _), sim_score in sim_list:\n",
        "            if lemma_name not in lemma_sim_dict or sim_score > lemma_sim_dict[lemma_name]:\n",
        "                if lemma_name != palabra_base: # Excluir la palabra base de sus propios similares\n",
        "                     lemma_sim_dict[lemma_name] = sim_score\n",
        "\n",
        "        sorted_lemmas = sorted(lemma_sim_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "        return sorted_lemmas\n",
        "\n",
        "    similares_path_unicos = unique_sorted_lemmas(similares_path)\n",
        "    similares_wup_unicos = unique_sorted_lemmas(similares_wup)\n",
        "\n",
        "    return similares_path_unicos[:top_n], similares_wup_unicos[:top_n]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G6J2rPE0J7L0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procesamiento para cada documento"
      ],
      "metadata": {
        "id": "uYLpHBvYKJgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if corpus and documentos_normalizados_wordnet:\n",
        "    print(\"\\n--- Calculando Similitud de Palabras con Synsets ---\")\n",
        "    for file_id in corpus.fileids():\n",
        "        print(f\"\\n--- Documento: {file_id} ---\")\n",
        "        tokens_doc = documentos_normalizados_wordnet.get(file_id, [])\n",
        "        if not tokens_doc:\n",
        "            print(\"No hay tokens normalizados para este documento.\")\n",
        "            continue\n",
        "\n",
        "        # --- Identificar Verbo Más Común ---\n",
        "        verbos_doc = [lema for lema, tag in tokens_doc if tag == wn.VERB]\n",
        "        if verbos_doc:\n",
        "            contador_verbos = Counter(verbos_doc)\n",
        "            verbo_mas_comun, freq_verbo = contador_verbos.most_common(1)[0]\n",
        "            print(f\"Verbo más común: '{verbo_mas_comun}' (Frecuencia: {freq_verbo})\")\n",
        "\n",
        "            path_sim_verbos, wup_sim_verbos = obtener_palabras_similares_wordnet(verbo_mas_comun, wn.VERB)\n",
        "            print(\"  Verbos similares (path_similarity):\")\n",
        "            for lema, sim in path_sim_verbos: # Ignoramos el synset name en la impresión\n",
        "              print(f\"    - {lema}: {sim:.4f}\")\n",
        "            if not path_sim_verbos: print(\"    No se encontraron verbos similares con path_similarity.\")\n",
        "\n",
        "            print(\"  Verbos similares (wup_similarity):\")\n",
        "            for lema, sim in wup_sim_verbos:\n",
        "              print(f\"    - {lema}: {sim:.4f}\")\n",
        "            if not wup_sim_verbos: print(\"    No se encontraron verbos similares con wup_similarity.\")\n",
        "        else:\n",
        "            print(\"No se encontraron verbos en este documento.\")\n",
        "\n",
        "        # --- Identificar Sustantivo Más Común ---\n",
        "        sustantivos_doc = [lema for lema, tag in tokens_doc if tag == wn.NOUN]\n",
        "        if sustantivos_doc:\n",
        "            contador_sustantivos = Counter(sustantivos_doc)\n",
        "            sustantivo_mas_comun, freq_sust = contador_sustantivos.most_common(1)[0]\n",
        "            print(f\"\\nSustantivo más común: '{sustantivo_mas_comun}' (Frecuencia: {freq_sust})\")\n",
        "\n",
        "            path_sim_sust, wup_sim_sust = obtener_palabras_similares_wordnet(sustantivo_mas_comun, wn.NOUN)\n",
        "            print(\"  Sustantivos similares (path_similarity):\")\n",
        "            for lema, sim in path_sim_sust:\n",
        "              print(f\"    - {lema}: {sim:.4f}\")\n",
        "            if not path_sim_sust: print(\"    No se encontraron sustantivos similares con path_similarity.\")\n",
        "\n",
        "            print(\"  Sustantivos similares (wup_similarity):\")\n",
        "            for lema, sim in wup_sim_sust:\n",
        "              print(f\"    - {lema}: {sim:.4f}\")\n",
        "            if not wup_sim_sust: print(\"    No se encontraron sustantivos similares con wup_similarity.\")\n",
        "        else:\n",
        "            print(\"No se encontraron sustantivos en este documento.\")\n",
        "else:\n",
        "    print(\"[INFO] No se puede proceder con la similitud de palabras con synsets. Corpus o documentos normalizados no disponibles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AdWDaI7KBW2",
        "outputId": "f1684375-b4ea-46d3-871b-e25a2be57f8c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculando Similitud de Palabras con Synsets ---\n",
            "\n",
            "--- Documento: doc1.txt ---\n",
            "Verbo más común: 'call' (Frecuencia: 8)\n",
            "  Verbos similares (path_similarity):\n",
            "    - label: 0.5000\n",
            "    - refer: 0.5000\n",
            "    - dub: 0.5000\n",
            "    - style: 0.5000\n",
            "    - baptize: 0.5000\n",
            "  Verbos similares (wup_similarity):\n",
            "    - refer: 0.8571\n",
            "    - dub: 0.8571\n",
            "    - style: 0.8571\n",
            "    - baptize: 0.8571\n",
            "    - rename: 0.8571\n",
            "\n",
            "Sustantivo más común: 'work' (Frecuencia: 11)\n",
            "  Sustantivos similares (path_similarity):\n",
            "    - action: 0.5000\n",
            "    - service: 0.5000\n",
            "    - wash: 0.5000\n",
            "    - care: 0.5000\n",
            "    - activity: 0.5000\n",
            "  Sustantivos similares (wup_similarity):\n",
            "    - action: 0.9333\n",
            "    - service: 0.9333\n",
            "    - wash: 0.9333\n",
            "    - care: 0.9333\n",
            "    - operation: 0.9333\n",
            "\n",
            "--- Documento: doc2.txt ---\n",
            "Verbo más común: 'make' (Frecuencia: 5)\n",
            "  Verbos similares (path_similarity):\n",
            "    - overdo: 0.5000\n",
            "    - breathe: 0.3333\n",
            "    - hold: 0.3333\n",
            "    - blow: 0.3333\n",
            "    - act: 0.3333\n",
            "  Verbos similares (wup_similarity):\n",
            "    - overdo: 0.6667\n",
            "    - breathe: 0.5000\n",
            "    - hold: 0.5000\n",
            "    - blow: 0.5000\n",
            "    - act: 0.5000\n",
            "\n",
            "Sustantivo más común: 'work' (Frecuencia: 13)\n",
            "  Sustantivos similares (path_similarity):\n",
            "    - action: 0.5000\n",
            "    - service: 0.5000\n",
            "    - wash: 0.5000\n",
            "    - care: 0.5000\n",
            "    - activity: 0.5000\n",
            "  Sustantivos similares (wup_similarity):\n",
            "    - action: 0.9333\n",
            "    - service: 0.9333\n",
            "    - wash: 0.9333\n",
            "    - care: 0.9333\n",
            "    - operation: 0.9333\n",
            "\n",
            "--- Documento: doc3.txt ---\n",
            "Verbo más común: 'write' (Frecuencia: 8)\n",
            "  Verbos similares (path_similarity):\n",
            "    - draw: 0.5000\n",
            "    - write_off: 0.5000\n",
            "    - verse: 0.5000\n",
            "    - dramatize: 0.5000\n",
            "    - rewrite: 0.5000\n",
            "  Verbos similares (wup_similarity):\n",
            "    - draw: 0.8571\n",
            "    - write_off: 0.8571\n",
            "    - verse: 0.8571\n",
            "    - dramatize: 0.8571\n",
            "    - rewrite: 0.8571\n",
            "\n",
            "Sustantivo más común: 'boccaccio' (Frecuencia: 23)\n",
            "  Sustantivos similares (path_similarity):\n",
            "    - poet: 0.5000\n",
            "    - bard: 0.3333\n",
            "    - Racine: 0.3333\n",
            "    - Marlowe: 0.3333\n",
            "    - elegist: 0.3333\n",
            "  Sustantivos similares (wup_similarity):\n",
            "    - poet: 0.9524\n",
            "    - bard: 0.9091\n",
            "    - Racine: 0.9091\n",
            "    - Marlowe: 0.9091\n",
            "    - elegist: 0.9091\n",
            "\n",
            "--- Documento: doc4.txt ---\n",
            "Verbo más común: 'present' (Frecuencia: 2)\n",
            "  Verbos similares (path_similarity):\n",
            "    - bring_home: 0.5000\n",
            "    - show: 0.5000\n",
            "    - give: 0.3333\n",
            "    - peep: 0.3333\n",
            "    - flash: 0.3333\n",
            "  Verbos similares (wup_similarity):\n",
            "    - bring_home: 0.8000\n",
            "    - breathe: 0.4000\n",
            "    - hold: 0.4000\n",
            "    - blow: 0.4000\n",
            "    - act: 0.4000\n",
            "\n",
            "Sustantivo más común: 'potion' (Frecuencia: 5)\n",
            "  Sustantivos similares (path_similarity):\n",
            "    - beverage: 0.5000\n",
            "    - elixir: 0.5000\n",
            "    - philter: 0.5000\n",
            "    - food: 0.3333\n",
            "    - cooler: 0.3333\n",
            "  Sustantivos similares (wup_similarity):\n",
            "    - elixir: 0.9474\n",
            "    - philter: 0.9474\n",
            "    - beverage: 0.9412\n",
            "    - elixir_of_life: 0.9000\n",
            "    - cooler: 0.8889\n",
            "\n",
            "--- Documento: doc5.txt ---\n",
            "Verbo más común: 'give' (Frecuencia: 8)\n",
            "  Verbos similares (path_similarity):\n",
            "    - infect: 0.5000\n",
            "    - deliver: 0.5000\n",
            "    - award: 0.5000\n",
            "    - breathe: 0.3333\n",
            "    - hold: 0.3333\n",
            "  Verbos similares (wup_similarity):\n",
            "    - infect: 0.6667\n",
            "    - deliver: 0.6667\n",
            "    - award: 0.6667\n",
            "    - breathe: 0.5000\n",
            "    - hold: 0.5000\n",
            "\n",
            "Sustantivo más común: 'sex' (Frecuencia: 20)\n",
            "  Sustantivos similares (path_similarity):\n",
            "    - perversion: 0.5000\n",
            "    - bondage: 0.5000\n",
            "    - outercourse: 0.5000\n",
            "    - safe_sex: 0.5000\n",
            "    - conception: 0.5000\n",
            "  Sustantivos similares (wup_similarity):\n",
            "    - perversion: 0.9231\n",
            "    - bondage: 0.9231\n",
            "    - outercourse: 0.9231\n",
            "    - safe_sex: 0.9231\n",
            "    - conception: 0.9231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Similitud de documentos con synsets (10 pts)\n",
        "\n",
        "Para esta sección:\n",
        "1.  Para cada libro, se extraerá la **frase más representativa** de su introducción. Se utilizará el algoritmo TextRank (a través de la biblioteca `sumy`), que fue una opción robusta en la Práctica 3.\n",
        "2.  De los cinco libros, se elegirá uno (por ejemplo, el primero, `doc1.txt`) y se comparará su frase representativa con las de los otros cuatro libros.\n",
        "3.  La similitud entre estas frases representativas se calculará utilizando `path_similarity` a nivel de documento (considerando las frases como mini-documentos)."
      ],
      "metadata": {
        "id": "hcCeckdgK25K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_frase_representativa_textrank(texto_documento, num_oraciones=1):\n",
        "    \"\"\"\n",
        "    Extrae la(s) frase(s) más representativa(s) de un texto usando TextRank.\n",
        "\n",
        "    Args:\n",
        "        texto_documento (str): El texto original del documento.\n",
        "        num_oraciones (int): Número de oraciones a extraer como resumen.\n",
        "\n",
        "    Returns:\n",
        "        str: La frase o frases más representativas concatenadas.\n",
        "             Retorna string vacío si no se puede generar resumen.\n",
        "    \"\"\"\n",
        "    if not texto_documento.strip():\n",
        "        return \"\"\n",
        "    try:\n",
        "        parser = PlaintextParser.from_string(texto_documento, SumyTokenizer(\"english\"))\n",
        "        summarizer = TextRankSummarizer()\n",
        "        resumen_oraciones = summarizer(parser.document, num_oraciones)\n",
        "        return \" \".join([str(oracion) for oracion in resumen_oraciones])\n",
        "    except Exception as e:\n",
        "        print(f\"Error al extraer frase con TextRank: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Funciones para similitud de documentos con Path Similarity (adaptadas de los ejemplos) ---\n",
        "\n",
        "def doc_to_synsets_wordnet(document_text, lematizador_obj, stop_words_set):\n",
        "    \"\"\"\n",
        "    Convierte un texto (documento/frase) en una lista de sus synsets de WordNet.\n",
        "    Utiliza la normalización definida previamente (tokenización, POS, lematización).\n",
        "    \"\"\"\n",
        "    tokens_normalizados = normalizar_documento_wordnet(document_text, lematizador_obj, stop_words_set)\n",
        "    synsets_lista = []\n",
        "    for lema, wn_tag in tokens_normalizados:\n",
        "        # Tomamos el primer synset (más común) para cada lema\n",
        "        ss = wn.synsets(lema, pos=wn_tag)\n",
        "        if ss:\n",
        "            synsets_lista.append(ss[0])\n",
        "    return synsets_lista\n",
        "\n",
        "def path_similarity_entre_synsets(lista_synsets1, lista_synsets2):\n",
        "    \"\"\"\n",
        "    Calcula una puntuación de similitud entre dos listas de synsets.\n",
        "    Para cada synset en lista1, encuentra el synset más similar en lista2\n",
        "    (usando path_similarity) y promedia estas puntuaciones máximas.\n",
        "    \"\"\"\n",
        "    puntuaciones_maximas = []\n",
        "    for s1 in lista_synsets1:\n",
        "        mejor_sim_para_s1 = 0.0\n",
        "        for s2 in lista_synsets2:\n",
        "            sim = s1.path_similarity(s2)\n",
        "            if sim is not None and sim > mejor_sim_para_s1:\n",
        "                mejor_sim_para_s1 = sim\n",
        "        if mejor_sim_para_s1 > 0: # Solo considerar si hay alguna similitud\n",
        "            puntuaciones_maximas.append(mejor_sim_para_s1)\n",
        "\n",
        "    if not puntuaciones_maximas:\n",
        "        return 0.0\n",
        "    return sum(puntuaciones_maximas) / len(puntuaciones_maximas)\n",
        "\n",
        "def document_path_similarity_wordnet(doc_text1, doc_text2, lematizador_obj, stop_words_set):\n",
        "    \"\"\"\n",
        "    Calcula la similitud entre dos textos usando path_similarity en sus synsets.\n",
        "    Es una medida simétrica: (sim(doc1, doc2) + sim(doc2, doc1)) / 2.\n",
        "    \"\"\"\n",
        "    synsets1 = doc_to_synsets_wordnet(doc_text1, lematizador_obj, stop_words_set)\n",
        "    synsets2 = doc_to_synsets_wordnet(doc_text2, lematizador_obj, stop_words_set)\n",
        "\n",
        "    if not synsets1 or not synsets2:\n",
        "        return 0.0 # Si uno de los documentos no tiene synsets, la similitud es 0.\n",
        "\n",
        "    sim1_a_2 = path_similarity_entre_synsets(synsets1, synsets2)\n",
        "    sim2_a_1 = path_similarity_entre_synsets(synsets2, synsets1)\n",
        "\n",
        "    return (sim1_a_2 + sim2_a_1) / 2.0\n",
        "\n",
        "# --- Extracción de frases representativas ---\n",
        "frases_representativas = {}\n",
        "if corpus:\n",
        "    print(\"\\n--- Extrayendo Frases Representativas con TextRank ---\")\n",
        "    for file_id in corpus.fileids():\n",
        "        texto_original_doc = corpus.raw(file_id)\n",
        "        frase_repr = extraer_frase_representativa_textrank(texto_original_doc, num_oraciones=1)\n",
        "        frases_representativas[file_id] = frase_repr\n",
        "        print(f\"Documento '{file_id}': \\\"{frase_repr}\\\"\")\n",
        "else:\n",
        "    print(\"[INFO] No se puede extraer frases representativas. Corpus no disponible.\")\n",
        "\n",
        "\n",
        "# --- Comparación de similitud de frases (doc1 vs otros) ---\n",
        "if corpus and frases_representativas and len(corpus.fileids()) > 1:\n",
        "    print(\"\\n--- Calculando Similitud de Frases Representativas con Path Similarity (WordNet) ---\")\n",
        "\n",
        "    ids_archivos = corpus.fileids()\n",
        "    id_base = ids_archivos[4] # Tomamos el primer documento como base\n",
        "    frase_base = frases_representativas.get(id_base)\n",
        "\n",
        "    if frase_base:\n",
        "        print(f\"Frase base (de '{id_base}'): \\\"{frase_base}\\\"\")\n",
        "        for i in range(1, len(ids_archivos)):\n",
        "            id_comparar = ids_archivos[i]\n",
        "            frase_comparar = frases_representativas.get(id_comparar)\n",
        "            if frase_comparar:\n",
        "                similitud = document_path_similarity_wordnet(frase_base, frase_comparar, lemmatizer, stop_words_english)\n",
        "                print(f\"  Similitud con frase de '{id_comparar}': {similitud:.4f}\")\n",
        "            else:\n",
        "                print(f\"  No hay frase representativa para '{id_comparar}'.\")\n",
        "    else:\n",
        "        print(f\"No se pudo obtener la frase base para '{id_base}'.\")\n",
        "else:\n",
        "    print(\"[INFO] No se puede calcular similitud de documentos con synsets. Corpus o frases no disponibles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3axtLPQK4-u",
        "outputId": "b52ed61a-55c3-440a-be7a-d4b7410fcb52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extrayendo Frases Representativas con TextRank ---\n",
            "Documento 'doc1.txt': \"The date of the 'Jayamangla' is fixed between the tenth and thirteenth centuries A.D., because while treating of the sixty-four arts an example is taken from the 'Kávyaprakásha,' which was written about the tenth century A.D. Again, the copy of the commentary procured was evidently a transcript of a manuscript which once had a place in the library of a Chaulukyan king named Vishaladeva, a fact elicited from the following sentence at the end of it:—\"\n",
            "Documento 'doc2.txt': \"By this is meant the desire on the part of the individual affected of desiring himself completely and unconditionally subject to the will of a person of the opposite sex, and being treated by this person as by a master, to be humiliated, abused, and tormented, even to the verge of death.\"\n",
            "Documento 'doc3.txt': \"Despite his complaints of the malevolence of his critics in the Proem to the Fourth Day of the Decameron, he had no lack of appreciation on the part of his fellow-citizens, and was employed by the Republic on several missions; to Bologna, probably with the view of averting the submission of that city to the Visconti in 1350; to Petrarch at Padua in March 1351, with a letter from the Priors announcing his restitution to citizenship, and inviting him to return to Florence, and assume the rectorship of the newly founded university; to Ludwig of Brandenburg with overtures for an alliance against the Visconti in December of the same year; and in the spring of 1354 to Pope Innocent VI.\"\n",
            "Documento 'doc4.txt': \"In its various mutations, its protean diversities, it is the love-potion, the philtre, the mystic concoction that, once quaffed, will instil love and passion and desire and lust, that will replenish erotic inadequacies, that will awaken the ancient fons vitae, the symbol of animate being, the source, as the antique Hellenes sensed and exemplified, of all cosmic creation, of the totality of living generation.\"\n",
            "Documento 'doc5.txt': \"We give to young folks, in their general education, as much as they can grasp of science and ethics and art, and yet in their sex education, which rightly has to do with all of these, we have said, “Give them only the bare physiological facts, lest they be prematurely stimulated.” Others of us, realizing that the bare physiological facts are shocking to many a sensitive child, and must somehow be softened with something pleasant, have said, “Give them the facts, yes, but see to it that they are so related to the wonders of evolution and the beauties of the natural world that the shock is minimized.” But none of us has yet dared to say, “Yes, give them the facts, give them the nature study, too, but also give them some conception of sex life as a vivifying joy, as a vital art, as a thing to be studied and developed with [Pg 5]reverence for its big meaning, with understanding of its far-reaching reactions, psychologically and spiritually, with temperant restraint, good taste and the highest idealism.” We have contented ourselves by assuming that marriage makes sex relations respectable.\"\n",
            "\n",
            "--- Calculando Similitud de Frases Representativas con Path Similarity (WordNet) ---\n",
            "Frase base (de 'doc5.txt'): \"We give to young folks, in their general education, as much as they can grasp of science and ethics and art, and yet in their sex education, which rightly has to do with all of these, we have said, “Give them only the bare physiological facts, lest they be prematurely stimulated.” Others of us, realizing that the bare physiological facts are shocking to many a sensitive child, and must somehow be softened with something pleasant, have said, “Give them the facts, yes, but see to it that they are so related to the wonders of evolution and the beauties of the natural world that the shock is minimized.” But none of us has yet dared to say, “Yes, give them the facts, give them the nature study, too, but also give them some conception of sex life as a vivifying joy, as a vital art, as a thing to be studied and developed with [Pg 5]reverence for its big meaning, with understanding of its far-reaching reactions, psychologically and spiritually, with temperant restraint, good taste and the highest idealism.” We have contented ourselves by assuming that marriage makes sex relations respectable.\"\n",
            "  Similitud con frase de 'doc2.txt': 0.2869\n",
            "  Similitud con frase de 'doc3.txt': 0.2379\n",
            "  Similitud con frase de 'doc4.txt': 0.2389\n",
            "  Similitud con frase de 'doc5.txt': 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Similitud de palabras con “embedding” (10 pts)\n",
        "\n",
        "En esta sección, se utilizará el modelo pre-entrenado de **GloVe “Wikipedia 2014 + Gigaword 5”** (específicamente, la versión con vectores de 50 dimensiones, `glove.6B.50d.txt`) para identificar, en cada documento, los 5 términos más similares al **verbo más frecuente** de ese mismo documento (identificado en el punto 3). La relación entre términos se calculará usando **similitud de coseno**.\n",
        "\n",
        "**Instrucciones para el alumno:**\n",
        "1.  Descarga los embeddings de GloVe \"Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d vectors)\". El archivo que necesitas es `glove.6B.50d.txt`.\n",
        "    *   Link de descarga: [GloVe Project Page](https://nlp.stanford.edu/projects/glove/) (busca `glove.6B.zip`).\n",
        "2.  Descomprime el archivo `glove.6B.zip`.\n",
        "3.  Coloca el archivo `glove.6B.50d.txt` en el mismo directorio que este Jupyter Notebook."
      ],
      "metadata": {
        "id": "rwp0Z3zBLnJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_glove_model(glove_file_path, expected_dim=50):\n",
        "    \"\"\"\n",
        "    Carga los embeddings de GloVe desde un archivo a un diccionario.\n",
        "\n",
        "    Args:\n",
        "        glove_file_path (str): Ruta al archivo de GloVe (ej. 'glove.6B.50d.txt').\n",
        "        expected_dim (int): La dimensionalidad esperada de los vectores GloVe.\n",
        "\n",
        "    Returns:\n",
        "        dict: Un diccionario donde las claves son palabras y los valores son sus vectores embedding (NumPy array),\n",
        "              o None si ocurre un error crítico como FileNotFoundError.\n",
        "    \"\"\"\n",
        "    print(f\"[INFO] Cargando modelo GloVe desde: {glove_file_path}...\")\n",
        "    embeddings_dict = {}\n",
        "    lines_skipped = 0\n",
        "    try:\n",
        "        with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f):\n",
        "                values = line.split()\n",
        "\n",
        "                if not values: # Manejar líneas vacías\n",
        "                    # print(f\"[DEBUG] Line {line_num+1}: Empty line. Skipping.\")\n",
        "                    lines_skipped +=1\n",
        "                    continue\n",
        "\n",
        "                word = values[0]\n",
        "\n",
        "                # Verificar si el número de valores es consistente con la dimensión esperada\n",
        "                if len(values) != expected_dim + 1:\n",
        "                    print(f\"[WARNING] Line {line_num+1}: Unexpected number of values for word '{word}'. Expected {expected_dim+1}, got {len(values)}. Skipping line: '{line.strip()[:100]}...'\")\n",
        "                    lines_skipped +=1\n",
        "                    continue\n",
        "                try:\n",
        "                    vector = np.asarray(values[1:], \"float32\")\n",
        "                    embeddings_dict[word] = vector\n",
        "                except ValueError as e:\n",
        "                    # Esto atrapará el error \"could not convert string to float\"\n",
        "                    print(f\"[WARNING] Line {line_num+1}: Could not convert vector values to float for word '{word}'. Error: {e}. Skipping line: '{line.strip()[:100]}...'\")\n",
        "                    lines_skipped +=1\n",
        "                    continue\n",
        "\n",
        "        if lines_skipped > 0:\n",
        "            print(f\"[INFO] Se omitieron {lines_skipped} líneas durante la carga del modelo GloVe debido a problemas de formato.\")\n",
        "\n",
        "        if not embeddings_dict:\n",
        "            print(\"[WARNING] No se cargaron embeddings. El diccionario está vacío. Verifica el archivo GloVe, la ruta y su contenido.\")\n",
        "        else:\n",
        "            print(f\"[SUCCESS] Modelo GloVe cargado. {len(embeddings_dict)} vectores de palabras.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[ERROR] Archivo GloVe no encontrado en '{glove_file_path}'.\")\n",
        "        print(\"Por favor, descarga 'glove.6B.50d.txt' y colócalo en el directorio correcto.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Ocurrió un error general al cargar GloVe: {e}\")\n",
        "        return None\n",
        "    return embeddings_dict\n",
        "\n",
        "\n",
        "# Asegúrate de que 'glove.6B.50d.txt' está en el mismo directorio que este notebook, o proporciona la ruta correcta.\n",
        "# Para Google Colab, primero debes subir el archivo.\n",
        "glove_file = '/content/glove.6B.50d.txt' # Ajusta esta ruta si es necesario\n",
        "glove_embeddings = cargar_glove_model(glove_file, expected_dim=50) # Se especifica la dimensión esperada\n",
        "\n",
        "def find_closest_glove_embeddings(palabra_objetivo, embeddings_dictionary, top_n=5):\n",
        "    \"\"\"\n",
        "    Encuentra las 'top_n' palabras más similares a 'palabra_objetivo' usando\n",
        "    embeddings GloVe y similitud coseno.\n",
        "\n",
        "    Args:\n",
        "        palabra_objetivo (str): La palabra para la cual buscar similares.\n",
        "        embeddings_dictionary (dict): Diccionario de palabras y sus embeddings GloVe.\n",
        "        top_n (int): Número de palabras similares a retornar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (palabra_similar, similitud_coseno), ordenada por similitud.\n",
        "    \"\"\"\n",
        "    if embeddings_dictionary is None:\n",
        "        print(\"[ERROR] El diccionario de embeddings GloVe no está cargado.\")\n",
        "        return []\n",
        "\n",
        "    palabra_objetivo_lower = palabra_objetivo.lower() # GloVe suele estar en minúsculas\n",
        "    if palabra_objetivo_lower not in embeddings_dictionary:\n",
        "        # print(f\"Advertencia: La palabra '{palabra_objetivo_lower}' no se encuentra en el vocabulario de GloVe.\")\n",
        "        return []\n",
        "\n",
        "    embedding_objetivo = embeddings_dictionary[palabra_objetivo_lower]\n",
        "    similitudes = {}\n",
        "\n",
        "    for word, embedding_candidato in embeddings_dictionary.items():\n",
        "        if word == palabra_objetivo_lower:\n",
        "            continue\n",
        "        # Similitud coseno = 1 - distancia coseno\n",
        "        sim_score = 1 - cosine(embedding_objetivo, embedding_candidato)\n",
        "        similitudes[word] = sim_score\n",
        "\n",
        "    palabras_mas_similares = sorted(similitudes.items(), key=lambda item: item[1], reverse=True)\n",
        "    return palabras_mas_similares[:top_n]\n",
        "\n",
        "# --- Procesamiento para cada documento (usando verbos más frecuentes del punto 3) ---\n",
        "if corpus and documentos_normalizados_wordnet and glove_embeddings:\n",
        "    print(\"\\n--- Calculando Similitud de Palabras con Embeddings GloVe (Verbos más frecuentes) ---\")\n",
        "    for file_id in corpus.fileids():\n",
        "        print(f\"\\n--- Documento: {file_id} ---\")\n",
        "        tokens_doc = documentos_normalizados_wordnet.get(file_id, [])\n",
        "        if not tokens_doc:\n",
        "            print(\"No hay tokens normalizados para este documento.\")\n",
        "            continue\n",
        "\n",
        "        verbos_doc = [lema for lema, tag in tokens_doc if tag == wn.VERB]\n",
        "        if verbos_doc:\n",
        "            contador_verbos = Counter(verbos_doc)\n",
        "            verbo_mas_comun, _ = contador_verbos.most_common(1)[0]\n",
        "            print(f\"Verbo más común (lema): '{verbo_mas_comun}'\")\n",
        "\n",
        "            similares_glove = find_closest_glove_embeddings(verbo_mas_comun, glove_embeddings, top_n=5)\n",
        "            if similares_glove:\n",
        "                print(f\"  Los 5 términos más similares a '{verbo_mas_comun}' según GloVe (coseno):\")\n",
        "                for palabra, sim in similares_glove:\n",
        "                    print(f\"    - {palabra}: {sim:.4f}\")\n",
        "            else:\n",
        "                print(f\"  No se encontraron palabras similares en GloVe para '{verbo_mas_comun}' o no está en el vocabulario.\")\n",
        "        else:\n",
        "            print(\"No se encontraron verbos en este documento para analizar con GloVe.\")\n",
        "else:\n",
        "    print(\"\\n[INFO] No se puede proceder con similitud de palabras GloVe. Corpus, documentos normalizados o GloVe no disponibles/cargados.\")\n",
        "    if not glove_embeddings:\n",
        "        print(\"[INFO] Específicamente, los embeddings de GloVe no se cargaron correctamente. Verifica el archivo y la ruta.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exb0yZpPLy49",
        "outputId": "a377a096-c6c5-4eed-88d2-2bada47fb550"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Cargando modelo GloVe desde: /content/glove.6B.50d.txt...\n",
            "[SUCCESS] Modelo GloVe cargado. 400000 vectores de palabras.\n",
            "\n",
            "--- Calculando Similitud de Palabras con Embeddings GloVe (Verbos más frecuentes) ---\n",
            "\n",
            "--- Documento: doc1.txt ---\n",
            "Verbo más común (lema): 'call'\n",
            "  Los 5 términos más similares a 'call' según GloVe (coseno):\n",
            "    - calls: 0.8872\n",
            "    - calling: 0.8769\n",
            "    - asking: 0.8592\n",
            "    - ask: 0.8572\n",
            "    - answer: 0.8378\n",
            "\n",
            "--- Documento: doc2.txt ---\n",
            "Verbo más común (lema): 'make'\n",
            "  Los 5 términos más similares a 'make' según GloVe (coseno):\n",
            "    - making: 0.9406\n",
            "    - take: 0.9393\n",
            "    - come: 0.9354\n",
            "    - give: 0.9349\n",
            "    - need: 0.9262\n",
            "\n",
            "--- Documento: doc3.txt ---\n",
            "Verbo más común (lema): 'write'\n",
            "  Los 5 términos más similares a 'write' según GloVe (coseno):\n",
            "    - writing: 0.8501\n",
            "    - read: 0.8217\n",
            "    - publish: 0.7848\n",
            "    - notes: 0.7766\n",
            "    - books: 0.7764\n",
            "\n",
            "--- Documento: doc4.txt ---\n",
            "Verbo más común (lema): 'present'\n",
            "  Los 5 términos más similares a 'present' según GloVe (coseno):\n",
            "    - same: 0.8586\n",
            "    - which: 0.8549\n",
            "    - of: 0.8475\n",
            "    - there: 0.8450\n",
            "    - however: 0.8408\n",
            "\n",
            "--- Documento: doc5.txt ---\n",
            "Verbo más común (lema): 'give'\n",
            "  Los 5 términos más similares a 'give' según GloVe (coseno):\n",
            "    - take: 0.9359\n",
            "    - make: 0.9349\n",
            "    - giving: 0.9313\n",
            "    - need: 0.8977\n",
            "    - put: 0.8923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Similitud de documentos con “embedding” (10 pts)\n",
        "\n",
        "Se realizará un procedimiento similar al del punto 4, pero esta vez utilizando el modelo **`bert-base-uncased`** para calcular la similitud entre las frases representativas.\n",
        "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje que genera embeddings contextuales, lo que significa que la representación de una palabra o frase depende de su contexto.\n",
        "\n",
        "1.  Se utilizarán las frases representativas extraídas en el punto 4.\n",
        "2.  Se obtendrá el embedding de cada frase utilizando `bert-base-uncased`.\n",
        "3.  Se elegirá la frase del primer documento (`doc1.txt`) como base y se calculará la similitud coseno con las frases de los otros cuatro documentos."
      ],
      "metadata": {
        "id": "cTyCtnZsMAlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(\"[INFO] Cargando tokenizer y modelo BERT (bert-base-uncased)...\")\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    bert_model.eval() # Poner el modelo en modo de evaluación (desactiva dropout, etc.)\n",
        "    print(\"[SUCCESS] Tokenizer y modelo BERT cargados.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] No se pudo cargar el modelo BERT: {e}\")\n",
        "    bert_tokenizer = None\n",
        "    bert_model = None\n",
        "\n",
        "\n",
        "def get_bert_sentence_embedding(sentence, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Obtiene el embedding de una frase usando un modelo BERT.\n",
        "    El embedding se toma del token [CLS] de la última capa oculta.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): La frase para la cual generar el embedding.\n",
        "        tokenizer (BertTokenizer): El tokenizer de BERT.\n",
        "        model (BertModel): El modelo BERT pre-entrenado.\n",
        "\n",
        "    Returns:\n",
        "        np.array: El vector de embedding de la frase, o None si hay error.\n",
        "    \"\"\"\n",
        "    if not sentence.strip(): # Manejar frases vacías\n",
        "        print(\"[WARNING] Se intentó obtener embedding BERT para una frase vacía.\")\n",
        "        return None\n",
        "    if tokenizer is None or model is None:\n",
        "        print(\"[ERROR] Tokenizer o modelo BERT no están disponibles para get_bert_sentence_embedding.\")\n",
        "        return None\n",
        "    try:\n",
        "        # Tokenizar la oración y añadir tokens especiales [CLS] y [SEP]\n",
        "        inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "\n",
        "        with torch.no_grad(): # Desactivar cálculo de gradientes para inferencia\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # El embedding del token [CLS] se usa a menudo como representación de toda la secuencia.\n",
        "        # outputs.last_hidden_state tiene forma (batch_size, sequence_length, hidden_size)\n",
        "        cls_embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy() # Tomar el embedding [CLS] del primer (y único) item en el batch\n",
        "        return cls_embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error al generar embedding BERT para la frase '{sentence[:50]}...': {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Obtener embeddings BERT para las frases representativas ---\n",
        "bert_embeddings_frases = {}\n",
        "# Se verifica 'frases_representativas' aquí. Si no está definida, se imprimirá el mensaje del 'else' más abajo.\n",
        "if 'frases_representativas' in locals() and frases_representativas and bert_tokenizer and bert_model:\n",
        "    print(\"\\n--- Generando Embeddings BERT para Frases Representativas ---\")\n",
        "    for file_id, frase in frases_representativas.items():\n",
        "        if frase: # Solo si hay frase\n",
        "            print(f\"Procesando frase de '{file_id}'...\")\n",
        "            embedding = get_bert_sentence_embedding(frase, bert_tokenizer, bert_model)\n",
        "            if embedding is not None:\n",
        "                bert_embeddings_frases[file_id] = embedding\n",
        "            else:\n",
        "                 print(f\"  No se pudo generar embedding para la frase de '{file_id}'.\")\n",
        "        else:\n",
        "            print(f\"  '{file_id}' no tiene frase representativa para procesar con BERT (frase vacía).\")\n",
        "else:\n",
        "    print(\"\\n[INFO] No se pueden generar embeddings BERT.\")\n",
        "    if 'frases_representativas' not in locals() or not frases_representativas:\n",
        "        print(\"[INFO] La variable 'frases_representativas' no está definida o está vacía. Asegúrate de ejecutar la Sección 4.\")\n",
        "    if not bert_tokenizer or not bert_model:\n",
        "        print(\"[INFO] El tokenizer o el modelo BERT no se cargaron correctamente.\")\n",
        "\n",
        "\n",
        "# --- Calcular similitud coseno entre embeddings BERT de las frases ---\n",
        "if corpus and 'frases_representativas' in locals() and frases_representativas and bert_embeddings_frases and len(corpus.fileids()) > 1:\n",
        "    print(\"\\n--- Calculando Similitud de Frases con Embeddings BERT (Coseno) ---\")\n",
        "\n",
        "    ids_archivos = corpus.fileids()\n",
        "    # Usaremos doc1.txt (índice 0) como base para la comparación,\n",
        "    # igual que se hizo en el ejemplo de la práctica para BERT.\n",
        "    id_base = ids_archivos[0]\n",
        "    embedding_base_bert = bert_embeddings_frases.get(id_base)\n",
        "    frase_base_texto = frases_representativas.get(id_base, \"N/A\")\n",
        "\n",
        "    if embedding_base_bert is not None:\n",
        "        print(f\"Frase base (de '{id_base}'): \\\"{frase_base_texto}\\\"\")\n",
        "        for i in range(len(ids_archivos)): # Iterar sobre todos, incluyendo la comparación consigo mismo si se desea, o filtrar.\n",
        "            id_comparar = ids_archivos[i]\n",
        "            if id_comparar == id_base and i != 0: # Evitar procesar el base dos veces si está en otra posición (poco probable con fileids())\n",
        "                continue\n",
        "            if i == 0 and id_base != ids_archivos[0]: # Si id_base no fue el primero, asegurarse de procesar el primero.\n",
        "                 # Este caso es más complejo de lo necesario si id_base siempre es ids_archivos[0]\n",
        "                 pass\n",
        "\n",
        "\n",
        "            embedding_comparar_bert = bert_embeddings_frases.get(id_comparar)\n",
        "            frase_comparar_texto = frases_representativas.get(id_comparar, \"N/A\")\n",
        "\n",
        "            if embedding_comparar_bert is not None:\n",
        "                if id_comparar == id_base: # Comparación consigo mismo\n",
        "                    similitud_bert = 1.0\n",
        "                else:\n",
        "                    # Similitud coseno = 1 - distancia coseno\n",
        "                    similitud_bert = 1 - cosine(embedding_base_bert, embedding_comparar_bert)\n",
        "\n",
        "                print(f\"  Similitud con frase de '{id_comparar}' (\\\"{frase_comparar_texto[:50]}...\\\"): {similitud_bert:.4f}\")\n",
        "            else:\n",
        "                # Solo imprimir si no es el documento base (ya que si el base no tiene embedding, se maneja antes)\n",
        "                if id_comparar != id_base:\n",
        "                    print(f\"  No hay embedding BERT para la frase de '{id_comparar}'.\")\n",
        "    else:\n",
        "        print(f\"No se pudo obtener el embedding BERT para la frase base de '{id_base}'.\")\n",
        "elif not bert_embeddings_frases and ('frases_representativas' in locals() and frases_representativas):\n",
        "    print(\"\\n[INFO] No se calculará similitud con BERT porque no se generaron embeddings para las frases (ver mensajes anteriores).\")\n",
        "else:\n",
        "    print(\"\\n[INFO] No se puede calcular similitud de documentos con BERT. Corpus, frases representativas, o embeddings BERT no disponibles.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdNwe1VVMC6M",
        "outputId": "2d08b93f-1075-410e-d8ee-04ebecebf34a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Cargando tokenizer y modelo BERT (bert-base-uncased)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] Tokenizer y modelo BERT cargados.\n",
            "\n",
            "--- Generando Embeddings BERT para Frases Representativas ---\n",
            "Procesando frase de 'doc1.txt'...\n",
            "Procesando frase de 'doc2.txt'...\n",
            "Procesando frase de 'doc3.txt'...\n",
            "Procesando frase de 'doc4.txt'...\n",
            "Procesando frase de 'doc5.txt'...\n",
            "\n",
            "--- Calculando Similitud de Frases con Embeddings BERT (Coseno) ---\n",
            "Frase base (de 'doc1.txt'): \"The date of the 'Jayamangla' is fixed between the tenth and thirteenth centuries A.D., because while treating of the sixty-four arts an example is taken from the 'Kávyaprakásha,' which was written about the tenth century A.D. Again, the copy of the commentary procured was evidently a transcript of a manuscript which once had a place in the library of a Chaulukyan king named Vishaladeva, a fact elicited from the following sentence at the end of it:—\"\n",
            "  Similitud con frase de 'doc1.txt' (\"The date of the 'Jayamangla' is fixed between the ...\"): 1.0000\n",
            "  Similitud con frase de 'doc2.txt' (\"By this is meant the desire on the part of the ind...\"): 0.7630\n",
            "  Similitud con frase de 'doc3.txt' (\"Despite his complaints of the malevolence of his c...\"): 0.6534\n",
            "  Similitud con frase de 'doc4.txt' (\"In its various mutations, its protean diversities,...\"): 0.7457\n",
            "  Similitud con frase de 'doc5.txt' (\"We give to young folks, in their general education...\"): 0.7806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis y Conclusiones\n",
        "\n",
        "Este experimento comparó diferentes enfoques para medir la similitud entre palabras y documentos (representados por frases). Se utilizaron WordNet (con `path_similarity` y `wup_similarity`), GloVe y BERT. Los documentos de entrada fueron introducciones de cinco libros del Proyecto Gutenberg con temáticas relacionadas con el amor, la sexualidad, y la literatura erótica/afectiva (Vatsyayana, Sacher-Masoch, Boccaccio, un texto sobre el \"motivo amatorio\" y pociones, y uno sobre educación sexual).\n",
        "\n",
        "### Similitud de Palabras:\n",
        "\n",
        "1.  **WordNet (`path_similarity` vs. `wup_similarity`):**\n",
        "    *   **Tipo de palabras similares:** Ambas métricas arrojaron palabras que son semánticamente cercanas en la jerarquía de WordNet, principalmente sinónimos, hiperónimos (más generales) o hipónimos (más específicos). Por ejemplo, para 'call', ambas encontraron 'refer', 'dub', 'style', 'baptize'. Para 'work', ambas encontraron 'action', 'service', 'care'.\n",
        "    *   **Diferencias en puntajes:** `wup_similarity` consistentemente produjo puntajes más altos que `path_similarity` para las mismas o similares parejas de palabras (e.g., 'call'-'refer': path=0.5000, wup=0.8571; 'work'-'action': path=0.5000, wup=0.9333). Esto se debe a que `wup_similarity` considera la profundidad de los synsets en la jerarquía y la profundidad de su ancestro común más específico (LCS). Si dos synsets son profundos (específicos) y su LCS también es profundo, `wup_similarity` les dará una alta puntuación, reflejando una cercanía conceptual específica. `path_similarity` se basa únicamente en la longitud de la ruta más corta, por lo que puede dar puntajes más bajos si la ruta es larga, incluso si los conceptos son conceptualmente cercanos y específicos.\n",
        "    *   **Intuitividad:** `wup_similarity` a menudo parece más intuitiva para reflejar la \"cercanía semántica\" porque premia la especificidad compartida. `path_similarity` es más directa pero puede no capturar tan bien la cercanía entre conceptos muy específicos si están separados por varios nodos intermedios más generales.\n",
        "    *   **Impacto de la estructura jerárquica:** Ambas métricas dependen fundamentalmente de la taxonomía de WordNet. `path_similarity` es una función inversa de la distancia en el grafo. `wup_similarity` utiliza la profundidad de los nodos y su LCS. Esto significa que los resultados están limitados por la estructura y el contenido de WordNet. Para 'boccaccio', WordNet lo relaciona con 'poet', 'bard', lo cual es correcto, mostrando su capacidad para manejar nombres propios si están en su lexicón como instancias de un tipo.\n",
        "\n",
        "2.  **GloVe (similitud coseno):**\n",
        "    *   **Tipo de palabras similares:** GloVe encontró palabras que tienden a co-ocurrir o usarse en contextos similares en su corpus de entrenamiento (Wikipedia + Gigaword). Estas pueden ser:\n",
        "        *   Formas flexionadas o derivadas: 'call' -> 'calls', 'calling'; 'make' -> 'making'; 'write' -> 'writing'; 'give' -> 'giving'.\n",
        "        *   Palabras temáticamente relacionadas o acciones/conceptos asociados: 'call' -> 'asking', 'ask', 'answer'; 'make' -> 'take', 'come', 'give', 'need'; 'write' -> 'read', 'publish', 'notes', 'books'.\n",
        "        *   Un caso particular fue para 'present' (verbo más común en doc4.txt, aunque solo con frecuencia 2), donde GloVe devolvió 'same', 'which', 'of', 'there', 'however'. Esto sugiere que la palabra 'present' en su forma base es muy común y co-ocurre con una amplia gama de palabras funcionales o estructurales en el vasto corpus de GloVe, y el embedding puede haber capturado este uso más general o su rol sintáctico en lugar de un significado semántico específico de \"presentar algo\".\n",
        "    *   **Comparación con WordNet:**\n",
        "        *   WordNet ofrece similitud basada en relaciones léxicas curadas (sinonimia, hiperonimia). GloVe ofrece similitud basada en distribución y contexto.\n",
        "        *   Para 'call', WordNet dio sinónimos de acción ('refer', 'dub'), mientras GloVe dio formas de la palabra y acciones relacionadas ('asking', 'answer').\n",
        "        *   Para 'write', WordNet dio acciones similares ('draw', 'verse'), GloVe dio elementos del ecosistema de la escritura ('read', 'publish', 'books').\n",
        "        *   GloVe captura aspectos diferentes de la similitud, más asociativos y contextuales, mientras WordNet es más taxonómico.\n",
        "    *   **Impacto del corpus de entrenamiento:** Los resultados de GloVe reflejan las asociaciones presentes en Wikipedia y Gigaword. Si el corpus fuera diferente (ej., literatura médica), las palabras similares también lo serían. El caso de 'present' es un buen ejemplo de cómo un corpus general puede llevar a asociaciones muy amplias para palabras comunes.\n",
        "\n",
        "3.  **Limitaciones Observadas:**\n",
        "    *   **WordNet:**\n",
        "        *   *Cobertura de vocabulario:* Aunque bueno, no es exhaustivo. Neologismos, jerga o términos muy especializados pueden faltar. 'boccaccio' fue encontrado, pero otros nombres propios o términos técnicos podrían no estar.\n",
        "        *   *Sensibilidad al contexto (limitada):* Aunque se usa POS tagging, WordNet opera principalmente a nivel de lema y su \"sentido más común\" o el primer synset. No captura el matiz contextual fino que una palabra puede tener en una frase específica.\n",
        "        *   *Dependencia de la calidad del preprocesamiento:* Errores en POS tagging o lematización afectan directamente la búsqueda en WordNet.\n",
        "    *   **GloVe:**\n",
        "        *   *Palabras Fuera de Vocabulario (OOV):* Aunque el modelo `glove.6B` tiene un vocabulario grande (400k), aún puede haber palabras OOV. Los verbos frecuentes analizados estaban presentes.\n",
        "        *   *Polisemia:* GloVe asigna un único vector a cada palabra (ej., \"bank\"). Si una palabra tiene múltiples significados, su vector es una mezcla de todos sus contextos, lo que puede llevar a resultados de similitud menos precisos para un sentido particular.\n",
        "        *   *Naturaleza de la similitud:* Como se vio con 'present', la similitud coseno puede reflejar co-ocurrencia frecuente con palabras funcionales más que una similitud semántica profunda para palabras muy comunes y polisémicas.\n",
        "\n",
        "### Similitud de Documentos (Frases Representativas):\n",
        "\n",
        "Se comparó una frase base con las frases representativas de los otros documentos.\n",
        "\n",
        "1.  **WordNet (`document_path_similarity`):**\n",
        "    *   **Interpretación de puntajes:** Se usó la frase de `doc5.txt` (\"We give to young folks... sex education...\") como base.\n",
        "        *   vs `doc2.txt` (Sacher-Masoch, \"...desire... subject to will...\"): 0.2869 (la más alta, lo cual es coherente ya que ambos tratan temas de sexualidad, aunque desde ángulos muy diferentes).\n",
        "        *   vs `doc3.txt` (Boccaccio): 0.2379 (la más baja, lo que tiene sentido ya que Boccaccio es más literario/histórico).\n",
        "        *   vs `doc4.txt` (amatory motif, pociones): 0.2389 (también baja, aunque habla de amor/deseo, es más abstracto/místico).\n",
        "    *   Los puntajes son relativamente bajos en general, lo cual es común cuando se promedian similitudes de `path_similarity` entre muchos synsets. Reflejan una similitud temática general, pero la señal puede ser débil.\n",
        "    *   **Impacto de la agregación:** El método promedia las mejores similitudes de synset a synset. Esto puede ser una simplificación excesiva, ya que pierde la estructura sintáctica de la frase y el significado composicional. La similitud general puede ser influenciada desproporcionadamente por unas pocas palabras clave muy similares o diluida por muchas palabras no coincidentes.\n",
        "\n",
        "2.  **BERT (similitud coseno sobre embeddings [CLS]):**\n",
        "    *   **Comparación con WordNet:** Se usó la frase de `doc1.txt` (\"The date of the 'Jayamangla'...\") como base.\n",
        "        *   vs `doc5.txt` (sex education): 0.7806 (la más alta). Doc1 (Vatsyayana/Kama Sutra) y doc5 (educación sexual) son temáticamente los más cercanos.\n",
        "        *   vs `doc2.txt` (Sacher-Masoch): 0.7630 (segunda más alta). También temáticamente relevante.\n",
        "        *   vs `doc4.txt` (amatory motif): 0.7457 (tercera). Relacionado con amor/deseo.\n",
        "        *   vs `doc3.txt` (Boccaccio): 0.6534 (la más baja). Coherente, al ser el más distante temáticamente (literario/narrativo).\n",
        "    *   Los puntajes de BERT son significativamente más altos y parecen ofrecer una discriminación más clara y semánticamente más coherente entre las frases.\n",
        "    *   **Reflejo del contexto:** BERT captura el contexto de manera bidireccional. El embedding [CLS] se considera una representación de toda la frase. Esto se reflejó en una evaluación de similitud que parece más alineada con la comprensión humana de las frases. El orden de similitud obtenido con BERT para `doc1.txt` es muy intuitivo dada la temática de los libros.\n",
        "    *   **Discrepancias:** Aunque las frases base fueron diferentes, la *calidad* de la clasificación de similitud de BERT parece superior. BERT probablemente identificaría patrones de similitud más robustos independientemente de la frase base elegida, siempre que las frases sean temáticamente distintas.\n",
        "\n",
        "3.  **Impacto de la Extracción de Frases (TextRank):**\n",
        "    *   Las frases extraídas por TextRank parecían ser resúmenes razonables de las introducciones. La calidad de esta frase es crucial. Si la frase extraída no es representativa, la comparación de similitud (para cualquier método) será defectuosa. Una frase más larga y detallada podría beneficiar a BERT, mientras que una frase con palabras clave claras podría ser suficiente para el enfoque de WordNet basado en synsets. En general, las frases eran lo suficientemente largas y distintivas para permitir una comparación significativa.\n",
        "\n",
        "### General:\n",
        "\n",
        "*   **Robustez/Utilidad:**\n",
        "    *   *Similitud de palabras:* WordNet es útil para encontrar relaciones léxicas explícitas (sinónimos, hiperónimos). GloVe es bueno para similitud asociativa/contextual. La elección depende del objetivo.\n",
        "    *   *Similitud de documentos:* BERT fue claramente más robusto y útil. Su capacidad para entender el contexto de la frase completa supera al enfoque de WordNet basado en la agregación de similitudes de palabras individuales.\n",
        "*   **Ventajas y Desventajas:**\n",
        "    *   **WordNet (Conocimiento Léxico):**\n",
        "        *   *Ventajas:* Relaciones semánticas explícitas y curadas, interpretable, no requiere entrenamiento (el recurso ya existe).\n",
        "        *   *Desventajas:* Cobertura limitada, estático (no se adapta a nuevos usos del lenguaje), sensible a la calidad del preprocesamiento (POS, lematización), dificultad para capturar significado composicional y contextual complejo.\n",
        "    *   **Embeddings (GloVe, BERT - Basados en Datos):**\n",
        "        *   *Ventajas:* Aprenden de grandes cantidades de texto, capturan matices y asociaciones no explícitas en lexicón. BERT, en particular, maneja bien el contexto y la polisemia (a nivel de frase).\n",
        "        *   *Desventajas:* Pueden ser \"cajas negras\" (difícil interpretar *por qué* dos cosas son similares). Requieren grandes datos y recursos computacionales para entrenamiento (aunque aquí usamos pre-entrenados). GloVe tiene un vector por palabra, lo que dificulta la polisemia. OOV puede ser un problema (menos para BERT debido a sub-palabras).\n",
        "\n",
        "**Desafíos Encontrados y Coherencia Temática:**\n",
        "*   El tema de los libros (amor, sexualidad, erotismo) proporcionó un buen campo de pruebas. Las similitudes encontradas, especialmente con BERT para documentos y con WordNet/GloVe para palabras como 'sex', 'potion', 'write', fueron en general coherentes con estas temáticas.\n",
        "*   El caso del verbo 'present' con GloVe fue un desafío interpretativo, mostrando que la \"similitud\" de GloVe no siempre es semántica directa, sino contextual amplia.\n",
        "*   La elección del \"primer synset\" en WordNet es una simplificación; un enfoque más sofisticado podría intentar desambiguar el sentido de la palabra en contexto antes de buscar similitudes, pero esto añade complejidad.\n",
        "\n",
        "En conclusión, los métodos basados en embeddings, y BERT en particular, demuestran una capacidad superior para capturar la similitud semántica a nivel de frase/documento debido a su comprensión contextual. WordNet sigue siendo valioso para analizar relaciones léxicas explícitas entre palabras individuales. GloVe ofrece un punto intermedio, capturando asociaciones contextuales a nivel de palabra. La elección de la herramienta depende de la tarea específica y del tipo de similitud que se desea medir."
      ],
      "metadata": {
        "id": "EUxQvzI9MN0f"
      }
    }
  ]
}